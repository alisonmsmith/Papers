
# Paper: Evaluating Visual Representations for Topic Understanding

Abstract:   Probabilistic topic models are important tools for indexing, summarizing, and
  analyzing large document collections by their themes.  However, promoting
  end-user understanding of topics remains an open research problem.  We compare
  labels generated by users given four topic visualization techniques---word
  lists, word lists with bars, word clouds, and network graphs---against each
  other and against automatically generated labels.  Our basis of comparison
  is participant ratings of how well labels describe documents from the topic.
  Our study has two phases: a labeling phase where participants label
  visualized topics and a validation phase where different participants select which labels
  best describe the topics' documents.  Although all visualizations produce
  similar quality labels, simple visualizations such as word lists allow participants to
  quickly understand topics, while complex visualizations take longer but expose
  multi-word expressions that simpler visualizations obscure.  Automatic labels
  lag behind user-created labels, but our dataset of manually labeled topics
  highlights linguistic patterns (e.g., hypernyms, phrases) that can be used to
  improve automatic topic labeling algorithms.

## Data

* **topics.txt**: ordered list of the 50 topics represented by their 20 topic words
* **algorithm.txt**: ordered list of the 50 algorithmically generated labels for the topics
* **labels.csv**: data collected from the Phase I study where users were given various topic visualizations and asked to provide short and long labels for them

| topicIdx | short label | confidence | usercode | created | long label | version | mode | key | cardinality | duration | randomImage_idx |
| --- | --- | --- | --- | --- | --- |--- | --- | --- | --- | --- | --- |
| id for the topic | short label provided for the topic | user specified confidence in their labeling | unique user code | created time | long label provided for the topic | **unused** | visualization type | **unused** | number of words displayed in the visualization | length of time to perform the task | image id |
*  **evaluation.csv**: data collected from the Phase II study where users were given four short labels plus an automatically generated label or four long labels and asked to determine the best and worst of the labels for a document set. 
  
| topicIdx | updated | usercode | created | shortOrLong | players | iter_num | done | key | cardinality | duration | worst | best |
| --- | --- | --- | --- | --- | --- |--- | --- | --- | --- | --- | --- | --- |
| id for the topic | updated time | unique user code | created time | whether short or long labels are being compared | the labels being compared (see details below) | **unused** | **unused** | **unused** | number of words displayed in the visualization | length of time to perform the task | the worst label | the best label |

The **players** and **best** and **worst** are defined by a unique identifier that looks like TOPICID-CARDINALITY-MODE-IMAGEID-SHORTORLONG, which refers to values of the labels.csv file. 

For example, the best label, **0-5-wordcloud-5-short**, refers to the short label in the labels.csv file for topic 0 with cardinality 5, random image id 5, and the word cloud, which is "cooking fish"

Also, it's important to note that the labels.csv file has extra entries that we did not include in the evaluation. Only the first five labels for each setting were used during the evaluation phase. 
